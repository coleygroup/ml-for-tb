{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "package_path = str(Path.cwd().parent)\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "from tbprop.deep_learning.modules import construct_fc_layers, MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_f1_score(y_true, y_score):\n",
    "    \"\"\" \n",
    "    Calculates the maximum possible F1 score given true labels and prob scores. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: List[int] or np.array\n",
    "        True (binary) labels.\n",
    "    y_score: List[float] or np.array\n",
    "        Prob scores from the model.\n",
    "    \"\"\"\n",
    "    return max([f1_score(y_true, (np.array(y_score) > t).astype(int)) for t in np.arange(0., 1.01, 0.02)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningBinaryClassifier(L.LightningModule):\n",
    "    def __init__(self, config={}):\n",
    "        \"\"\"\n",
    "        Constructor. Config should at least have the following params: \n",
    "        'learning_rate': float\n",
    "\n",
    "        To construct a lightning classifier, inherit this class and override the \n",
    "        following functions:\n",
    "            __init__()\n",
    "            forward()\n",
    "            predict_step() (Optional): use to create embeddings.\n",
    "            configure_optimizers (Optional): to use an alternate optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pos_weight = config.get('pos_weight', 1.)\n",
    "        self.learning_rate = config.get('learning_rate', 1e-1)\n",
    "        self.scale_loss = config.get('scale_loss', False)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.FloatTensor([self.pos_weight])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward function connecting torch.nn.Modules. \"\"\"\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\" Batch-wise training function. \"\"\"\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs).reshape(-1)\n",
    "\n",
    "        multiplier = 1/inputs.shape[0] if self.scale_loss else 1\n",
    "        train_loss = multiplier*self.loss_fn(outputs, targets.to(torch.float32))\n",
    "\n",
    "        self.log(\"ptl/train_loss\", train_loss, on_step=True, \n",
    "                 on_epoch=True, prog_bar=True, logger=True)\n",
    "        return train_loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\" Calculate batch-wise validation metrics at the end of each epoch. \"\"\"\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs).reshape(-1)\n",
    "\n",
    "        multiplier = 1/inputs.shape[0] if self.scale_loss else 1\n",
    "        loss = multiplier*self.loss_fn(outputs, targets.to(torch.float32))\n",
    "\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        auroc = roc_auc_score(targets, outputs)\n",
    "        ap = average_precision_score(targets, outputs)\n",
    "        max_f1 = max_f1_score(targets, outputs)\n",
    "\n",
    "        self.log(\"ptl/val_loss\", loss)\n",
    "        self.log(\"ptl/val_auroc\",auroc)\n",
    "        self.log(\"ptl/val_ap\", ap)\n",
    "        self.log(\"ptl/val_f1_score\", max_f1)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\" Calculate batch-wise validation metrics at the end of each epoch. \"\"\"\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs).reshape(-1)\n",
    "\n",
    "        multiplier = 1/inputs.shape[0] if self.scale_loss else 1\n",
    "        loss = multiplier*self.loss_fn(outputs, targets.to(torch.float32))\n",
    "\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        auroc = roc_auc_score(targets, outputs)\n",
    "        ap = average_precision_score(targets, outputs)\n",
    "        max_f1 = max_f1_score(targets, outputs)\n",
    "\n",
    "        self.log(\"ptl/test_loss\", loss)\n",
    "        self.log(\"ptl/test_auroc\", auroc)\n",
    "        self.log(\"ptl/test_ap\", ap)\n",
    "        self.log(\"ptl/test_f1_score\", max_f1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Configure optimizer. \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class FullyConnectedBinaryClassifier(LightningBinaryClassifier):\n",
    "    def __init__(self, config={}):\n",
    "        \"\"\"\n",
    "        Structure: Embedding -> Flatten -> MLP (x m)\n",
    "        Hyperparameters:\n",
    "            seq_len: int\n",
    "            embed_size: int\n",
    "            vocab_size: int\n",
    "            fc_num_layers: int\n",
    "            dropout_rate: int\n",
    "            learning_rate: float\n",
    "        \"\"\"\n",
    "        super().__init__(config=config)\n",
    "\n",
    "        self.seq_len = config.get('seq_len', 512) # Based on ChemBERTa\n",
    "        self.embed_size = config.get('embed_size', 64)\n",
    "        self.vocab_size = config.get('vocab_size', 591) # Based on ChemBERTa\n",
    "        self.dropout_rate = config.get('dropout_rate', 0.25)\n",
    "        self.fc_num_layers = config.get('fc_num_layers', 5)\n",
    "        self.fc_end_dim = 1\n",
    "        self.pad_index = 0\n",
    "\n",
    "        self.example_input_array = torch.randint(low=0, \n",
    "                                                 high=self.vocab_size, \n",
    "                                                 size=(1, self.seq_len))\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(self.vocab_size, \n",
    "                                            self.embed_size, \n",
    "                                            padding_idx=self.pad_index)\n",
    "        self.feature_extractor = torch.nn.Sequential(OrderedDict([('flatten', torch.nn.Flatten())]))\n",
    "\n",
    "        self.fc_start_dim = self.feature_extractor(torch.zeros(1, self.seq_len, self.embed_size)).shape[1]\n",
    "\n",
    "        self.fc_layer_sizes = construct_fc_layers(self.fc_start_dim, \n",
    "                                                  self.fc_num_layers, \n",
    "                                                  self.fc_end_dim)\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(self.fc_layer_sizes, \n",
    "                                        dropout_rate=self.dropout_rate)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x.squeeze()\n",
    "        return x\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        \"\"\" Use this as an embedding function. \"\"\"\n",
    "        x = self.embedding(batch)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 3/3 [00:00<00:00, 80.27it/s]\n"
     ]
    }
   ],
   "source": [
    "model = FullyConnectedBinaryClassifier({'seq_len': 128, 'fc_num_layers': 1})\n",
    "batch = torch.randint(0, 591, (3, 10, 128))\n",
    "trainer = L.Trainer(callbacks=[\n",
    "    ModelCheckpoint(dirpath=\"../data/checkpoints/pytorch_lightning\",\n",
    "                    filename=\"fcnn_best_val_loss_{epoch}\",\n",
    "                    monitor=\"ptl/val_loss\",\n",
    "                    mode=\"min\")\n",
    "])\n",
    "\n",
    "test_predictions = torch.cat(trainer.predict(model, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42967474, 0.45906004, 0.43000174, 0.4293967 , 0.65963835,\n",
       "       0.36255294, 0.43719235, 0.48620018, 0.32182297, 0.50228196,\n",
       "       0.36033338, 0.6073804 , 0.4058569 , 0.5296254 , 0.46441895,\n",
       "       0.43309945, 0.60983056, 0.57576287, 0.40438625, 0.5171854 ,\n",
       "       0.6859662 , 0.6818627 , 0.60502887, 0.57953286, 0.4866189 ,\n",
       "       0.5859223 , 0.62696993, 0.5145324 , 0.6307675 , 0.39782944],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
