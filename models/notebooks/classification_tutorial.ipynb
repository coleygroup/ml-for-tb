{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65d6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "/Users/vedang/miniconda3/envs/metal2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/Users/vedang/miniconda3/envs/metal2/lib/python3.10/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "package_path = str(Path.cwd().parent)\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append(package_path)\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import optuna\n",
    "\n",
    "from tbprop.metrics import binary_classification_metrics\n",
    "from tbprop.tree_based.preprocessing import VarianceThresholdPandas, StandardScalerPandas, \\\n",
    "        FeatureImportanceScoreThreshold, CorrelationThreshold\n",
    "from tbprop.model_comparison import compare_models_optimizers_on_split\n",
    "\n",
    "from rdkit.rdBase import BlockLogs\n",
    "\n",
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6c5206-ff76-47e2-bae2-97a741845660",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "TRN_PATH = f\"{DATA_DIR}/pk/pk_trn.csv\"\n",
    "VAL_PATH = f\"{DATA_DIR}/pk/pk_val.csv\"\n",
    "TST_PATH = f\"{DATA_DIR}/pk/pk_tst.csv\"\n",
    "\n",
    "MODE = 'bin_class'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "assert MODE in ('bin_class', 'reg', 'cat_class'), \\\n",
    "    f\"MODE, '{MODE}' is invalid.\"\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8619a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(TRN_PATH)\n",
    "df_val = pd.read_csv(VAL_PATH)\n",
    "df_tst = pd.read_csv(TST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72854a1c",
   "metadata": {},
   "source": [
    "For simplicity, we'll remove all other columns from the dataset and assume it just has two columns, the column containing the SMILES string, and the column containing the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e509f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df_trn[['smiles', 'auc_bin']]\n",
    "df_val = df_val[['smiles', 'auc_bin']]\n",
    "df_tst = df_tst[['smiles', 'auc_bin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b97fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>auc_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=[N+]([O-])c1oc(C(=O)NC2CN(C(=O)c3[nH]c4c(c3)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=[N+]([O-])c1sc(/C=N/Nc2nc(NC(C)C)nc(NC(C)C)n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clc1ccc(CCC(=O)c2c(O)cc(O)cc2[O-])cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=[N+]([O-])c1oc(CN=Nc2nc(Nc3ccccc3)cc(Nc3cccc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S(=O)(=O)(Nc1cc2c(C)c(C(=O)N3CC4(C3)CCCCC4)[nH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  auc_bin\n",
       "0  O=[N+]([O-])c1oc(C(=O)NC2CN(C(=O)c3[nH]c4c(c3)...        0\n",
       "1  O=[N+]([O-])c1sc(/C=N/Nc2nc(NC(C)C)nc(NC(C)C)n...        1\n",
       "2              Clc1ccc(CCC(=O)c2c(O)cc(O)cc2[O-])cc1        0\n",
       "3  O=[N+]([O-])c1oc(CN=Nc2nc(Nc3ccccc3)cc(Nc3cccc...        0\n",
       "4  S(=O)(=O)(Nc1cc2c(C)c(C(=O)N3CC4(C3)CCCCC4)[nH...        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ab648",
   "metadata": {},
   "source": [
    "We'll use `deepchem` to add some features. Here we're using RDKit descriptors and circular fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309d2a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating circular fingerprints...\n",
      "Generating rdkit fingerprints...\n",
      "Done.\n",
      "Shape of trn set = (114, 2260)\n",
      "Shape of val set = (38, 2260)\n",
      "Shape of tst set = (38, 2260)\n"
     ]
    }
   ],
   "source": [
    "featurizers = {\n",
    "    'circular': dc.feat.CircularFingerprint(size=2048, radius=4), \n",
    "    'rdkit': dc.feat.RDKitDescriptors()\n",
    "}\n",
    "\n",
    "def featurize(df, key, featurizer):\n",
    "    feats = featurizer.featurize(df['smiles'])\n",
    "    pd_feats = pd.DataFrame(feats, columns=[key + '_' + str(i+1) for i in range(feats.shape[1])])\n",
    "    return pd.concat([df, pd_feats], axis=1)\n",
    "\n",
    "with BlockLogs():\n",
    "    for k, f in featurizers.items():\n",
    "        print(f\"Generating {k} fingerprints...\")\n",
    "        df_trn = featurize(df_trn, k, f)\n",
    "        df_val = featurize(df_val, k, f)\n",
    "        df_tst = featurize(df_tst, k, f)\n",
    "    print(\"Done.\")\n",
    "\n",
    "print(f\"Shape of trn set = {df_trn.shape}\")\n",
    "print(f\"Shape of val set = {df_val.shape}\")\n",
    "print(f\"Shape of tst set = {df_tst.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4e2880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2260)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1718a9",
   "metadata": {},
   "source": [
    "#### Using the validation set for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b51cac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of trn set = (114, 53)\n",
      "Final shape of val set = (38, 53)\n",
      "Final shape of tst set = (38, 53)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('var_thresh', VarianceThresholdPandas(threshold=.16)),\n",
    "    ('corr_thresh', CorrelationThreshold(threshold=.95)),\n",
    "    ('feat_imp_thresh', FeatureImportanceScoreThreshold(threshold=0., classifier='XGBoost', n_folds=3)),\n",
    "    ('scaler', StandardScalerPandas())\n",
    "])\n",
    "\n",
    "X_trn, y_trn = df_trn.drop(['smiles', 'auc_bin'], axis=1), df_trn['auc_bin']\n",
    "X_val, y_val = df_val.drop(['smiles', 'auc_bin'], axis=1), df_val['auc_bin']\n",
    "X_tst, y_tst = df_tst.drop(['smiles', 'auc_bin'], axis=1), df_tst['auc_bin']\n",
    "\n",
    "X_trn_proc = pipeline.fit_transform(X_trn, y_trn)\n",
    "X_val_proc = pipeline.transform(X_val)\n",
    "X_tst_proc = pipeline.transform(X_tst)\n",
    "\n",
    "print(f\"Final shape of trn set = {X_trn_proc.shape}\")\n",
    "print(f\"Final shape of val set = {X_val_proc.shape}\")\n",
    "print(f\"Final shape of tst set = {X_tst_proc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11b743e-fb2a-4833-85b0-f4ebe038668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models using 'hyperopt'.\n",
      "\n",
      "Optimizing: RandomForestClassifier.\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.13trial/s, best loss: -0.89197]\n",
      "Optimizing: XGBClassifier.\n",
      "100%|██████████| 20/20 [00:01<00:00, 14.74trial/s, best loss: -0.74377]\n",
      "Optimizing: LGBMClassifier.\n",
      "100%|██████████| 20/20 [00:00<00:00, 32.69trial/s, best loss: -0.8642699999999999]\n",
      "Optimizing: CatBoostClassifier.\n",
      "100%|██████████| 20/20 [00:35<00:00,  1.75s/trial, best loss: -0.83934]\n",
      "\n",
      "Model: RandomForestClassifier | Test AUROC: 0.89196675900277, AP: 0.811636698309747, Max F1: 0.8780487804878049\n",
      "Model: XGBClassifier | Test AUROC: 0.8670360110803323, AP: 0.8955950199583473, Max F1: 0.7878787878787878\n",
      "Model: LGBMClassifier | Test AUROC: 0.9030470914127424, AP: 0.9093003212365276, Max F1: 0.8717948717948718\n",
      "Model: CatBoostClassifier | Test AUROC: 0.9141274238227147, AP: 0.9061588983118938, Max F1: 0.8837209302325582\n",
      "\n",
      "Optimizer time = 59.852 s.\n",
      "\n",
      "Training models using 'optuna'.\n",
      "\n",
      "Optimizing: RandomForestClassifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: -0.93906: 100%|██████████| 20/20 [00:03<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing: XGBClassifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: -0.94737: 100%|██████████| 20/20 [00:09<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing: LGBMClassifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: -0.93906: 100%|██████████| 20/20 [00:00<00:00, 24.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing: CatBoostClassifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: -0.98615: 100%|██████████| 20/20 [00:21<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: RandomForestClassifier | Test AUROC: 0.8975069252077562, AP: 0.8926143879023949, Max F1: 0.8500000000000001\n",
      "Model: XGBClassifier | Test AUROC: 0.8975069252077562, AP: 0.879500199754484, Max F1: 0.8500000000000001\n",
      "Model: LGBMClassifier | Test AUROC: 0.8947368421052632, AP: 0.9099008610475936, Max F1: 0.8205128205128205\n",
      "Model: CatBoostClassifier | Test AUROC: 0.9252077562326869, AP: 0.9138707353877629, Max F1: 0.9\n",
      "\n",
      "Optimizer time = 59.474 s.\n",
      "\n",
      "Training models using 'random_search'.\n",
      "\n",
      "Optimizing: RandomForestClassifier.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "  Time taken = 4s\n",
      "Optimizing: XGBClassifier.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "  Time taken = 1s\n",
      "Optimizing: LGBMClassifier.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "  Time taken = 0s\n",
      "Optimizing: CatBoostClassifier.\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "  Time taken = 18s\n",
      "\n",
      "Model: RandomForestClassifier | Test AUROC: 0.8836565096952909, AP: 0.8338454398605939, Max F1: 0.8444444444444443\n",
      "Model: XGBClassifier | Test AUROC: 0.8919667590027701, AP: 0.888599201795796, Max F1: 0.8372093023255814\n",
      "Model: LGBMClassifier | Test AUROC: 0.5, AP: 0.5, Max F1: 0.6666666666666666\n",
      "Model: CatBoostClassifier | Test AUROC: 0.8698060941828254, AP: 0.8994165693438843, Max F1: 0.7894736842105263\n",
      "\n",
      "Optimizer time = 28.5 s.\n",
      "\n",
      "Total time = 147.826 s.\n"
     ]
    }
   ],
   "source": [
    "models = [RandomForestClassifier, XGBClassifier, LGBMClassifier, CatBoostClassifier]\n",
    "\n",
    "trained_models, test_metrics = \\\n",
    "    compare_models_optimizers_on_split(models, \n",
    "                                       X_trn_proc, y_trn, \n",
    "                                       X_tst_proc, y_tst, \n",
    "                                       X_val=X_val_proc, y_val=y_val,\n",
    "                                       random_state=RANDOM_SEED,\n",
    "                                       random_seed=RANDOM_SEED,\n",
    "                                       max_evals=20,\n",
    "                                       val_mode='fixed_split',\n",
    "                                       pipeline_suffix='P1',\n",
    "                                       optimizer_types=['hyperopt', 'optuna', 'random_search'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f5bc2",
   "metadata": {},
   "source": [
    "Following this you can save the model, results, and pipeline. I've commented it out so as not to overwrite files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fcda8f-ebac-4e20-bc28-e27ae610b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving data\n",
    "# with open(f\"{DATA_DIR}/trained_models.pkl\", 'wb') as f:\n",
    "#     pickle.dump(\n",
    "#         {\n",
    "#             'trained_models': trained_models,\n",
    "#             'test_metrics': test_metrics,\n",
    "#             'pipeline': pipeline\n",
    "#         },\n",
    "#         f\n",
    "#     )\n",
    "\n",
    "# Reading back data\n",
    "# with open(f\"{DATA_DIR}/trained_models.pkl\", 'rb') as f:\n",
    "#     metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4890bc",
   "metadata": {},
   "source": [
    "#### Using k-fold cross validation for hyperparameter tuning\n",
    "\n",
    "Alternatively, you can use k-fold CV for hyperparameter tuning. In this case, we don't need a validation set so we mix it back with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc5953a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, y_trn = df_trn.drop(['smiles', 'auc_bin'], axis=1), df_trn['auc_bin']\n",
    "X_val, y_val = df_val.drop(['smiles', 'auc_bin'], axis=1), df_val['auc_bin']\n",
    "X_tst, y_tst = df_tst.drop(['smiles', 'auc_bin'], axis=1), df_tst['auc_bin']\n",
    "\n",
    "train_X, train_y = pd.concat([X_trn, X_val]), pd.concat([y_trn, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4a63f",
   "metadata": {},
   "source": [
    "During k-fold CV, for each validation split, we need to call `fit_transform()` on the training data and `transform()` on the validation data. This means that the entire pipeline (and not just the model) needs to be fit in each of the k iterations of k-fold CV. Scikit-learn allows us to wrap the `pipeline` in the `cross_validate` function to achieve this. At the end of the pipeline, we'll add our classifier, and given a new suggestion for the classifier parameters, we should be able to initialize the entire pipeline with those parameters. This is done in the function below.\n",
    "\n",
    "In the future, even pipeline parameters can be set through hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dfd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(params):\n",
    "    pipeline = Pipeline([\n",
    "        ('var_thresh', VarianceThresholdPandas(threshold=.16)),\n",
    "        ('corr_thresh', CorrelationThreshold(threshold=.95)),\n",
    "        ('feat_imp_thresh', FeatureImportanceScoreThreshold(threshold=0., classifier='XGBoost', n_folds=3)),\n",
    "        ('scaler', StandardScalerPandas()),\n",
    "        ('classifier', XGBClassifier(**params))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def build_params(trial: optuna.trial.Trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.01),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5a7b1",
   "metadata": {},
   "source": [
    "The next step is to build an \"optimizer\" class. Building a class is not necessary, you can simply create a study, an objective function, and a search space and use it, but building a class is good practice from an object-oriented perspective (encapsulation of data and interfacing). Overall the steps here are:\n",
    "\n",
    "1. Initialize a study and define an evaluation metric.\n",
    "2. Define a search space (`_build_params`).\n",
    "3. Define an objective function (`objective`).\n",
    "4. Run the study until either `n_trials` or `timeout` number of seconds are reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca1cf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaOptimizer:\n",
    "    def __init__(self, X_trn, y_trn, eval_metric='roc_auc', n_folds=5):\n",
    "        \"\"\" Optuna optimizer. \"\"\"\n",
    "        self.X_trn = X_trn\n",
    "        self.y_trn = y_trn\n",
    "        self.eval_metric = eval_metric\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def objective(self, trial):\n",
    "        \"\"\" Objective function for Optuna. \"\"\"\n",
    "        params = build_params(trial)\n",
    "        clf = build_pipeline(params)\n",
    "        res = cross_validate(clf, self.X_trn, self.y_trn, scoring=self.eval_metric, cv=self.n_folds)\n",
    "        return res['test_score'].mean()\n",
    "    \n",
    "    def _name_study(self):\n",
    "        \"\"\" Generate a unique name for the study. \"\"\"\n",
    "        return f'optuna_{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "    def optimize(self, n_trials=100, timeout=600, show_progress_bar=True):\n",
    "        \"\"\" Run the optimization. \"\"\"\n",
    "        study = optuna.create_study(\n",
    "            study_name=self._name_study(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "        )\n",
    "\n",
    "        study.optimize(self.objective, \n",
    "                       n_trials=n_trials, \n",
    "                       timeout=timeout, \n",
    "                       show_progress_bar=show_progress_bar)\n",
    "        \n",
    "        return study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f896a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:51:09,938] A new study created in memory with name: optuna_20240509-105109\n",
      "Best trial: 0. Best value: 0.788944:  10%|█         | 1/10 [00:11<01:47, 11.93s/it, 11.93/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:51:21,871] Trial 0 finished with value: 0.7889444444444444 and parameters: {'n_estimators': 138, 'max_depth': 13, 'min_child_weight': 0.3765168195645399, 'learning_rate': 0.006804991368905178, 'subsample': 0.8907299462217086}. Best is trial 0 with value: 0.7889444444444444.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.798389:  20%|██        | 2/10 [00:23<01:31, 11.43s/it, 23.01/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:51:32,948] Trial 1 finished with value: 0.7983888888888889 and parameters: {'n_estimators': 160, 'max_depth': 6, 'min_child_weight': 0.8108615632945625, 'learning_rate': 0.004774108094506331, 'subsample': 0.6533436118206156}. Best is trial 1 with value: 0.7983888888888889.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.805333:  30%|███       | 3/10 [00:33<01:18, 11.16s/it, 33.86/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:51:43,798] Trial 2 finished with value: 0.8053333333333332 and parameters: {'n_estimators': 274, 'max_depth': 6, 'min_child_weight': 0.3966334368527861, 'learning_rate': 0.00659812264081889, 'subsample': 0.4607695159730384}. Best is trial 2 with value: 0.8053333333333332.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  40%|████      | 4/10 [00:45<01:09, 11.54s/it, 45.97/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:51:55,910] Trial 3 finished with value: 0.8156111111111111 and parameters: {'n_estimators': 375, 'max_depth': 19, 'min_child_weight': 0.21375087556986233, 'learning_rate': 0.008702722137710808, 'subsample': 0.4950761216908107}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  50%|█████     | 5/10 [00:57<00:57, 11.56s/it, 57.57/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:52:07,508] Trial 4 finished with value: 0.7797777777777777 and parameters: {'n_estimators': 306, 'max_depth': 15, 'min_child_weight': 0.9147100078398963, 'learning_rate': 0.007769169096986727, 'subsample': 0.32275745743890955}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  60%|██████    | 6/10 [01:09<00:46, 11.60s/it, 69.23/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:52:19,175] Trial 5 finished with value: 0.7335555555555555 and parameters: {'n_estimators': 421, 'max_depth': 9, 'min_child_weight': 0.7907626295697371, 'learning_rate': 0.005032792041487458, 'subsample': 0.17305616763778883}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  70%|███████   | 7/10 [01:21<00:35, 11.84s/it, 81.59/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:52:31,526] Trial 6 finished with value: 0.7898333333333333 and parameters: {'n_estimators': 362, 'max_depth': 10, 'min_child_weight': 0.23713399917303463, 'learning_rate': 0.0098761490524765, 'subsample': 0.27869323421555026}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  80%|████████  | 8/10 [01:33<00:23, 11.88s/it, 93.54/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:52:43,482] Trial 7 finished with value: 0.7949444444444445 and parameters: {'n_estimators': 378, 'max_depth': 12, 'min_child_weight': 0.19875781439395948, 'learning_rate': 0.0036547254082179175, 'subsample': 0.9374497812267503}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.815611:  90%|█████████ | 9/10 [01:44<00:11, 11.45s/it, 104.06/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:52:53,996] Trial 8 finished with value: 0.6907222222222222 and parameters: {'n_estimators': 392, 'max_depth': 11, 'min_child_weight': 0.7283640112669343, 'learning_rate': 0.004970358958154457, 'subsample': 0.10685813231177621}. Best is trial 3 with value: 0.8156111111111111.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.817111: 100%|██████████| 10/10 [01:55<00:00, 11.58s/it, 115.78/600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-05-09 10:53:05,720] Trial 9 finished with value: 0.8171111111111113 and parameters: {'n_estimators': 454, 'max_depth': 11, 'min_child_weight': 0.6772319601101815, 'learning_rate': 0.004773750316364623, 'subsample': 0.7488515680732578}. Best is trial 9 with value: 0.8171111111111113.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = OptunaOptimizer(X_trn=train_X, y_trn=train_y, eval_metric=\"roc_auc\", n_folds=5)\n",
    "best_trial = optimizer.optimize(n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46acf62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 454,\n",
       " 'max_depth': 11,\n",
       " 'min_child_weight': 0.6772319601101815,\n",
       " 'learning_rate': 0.004773750316364623,\n",
       " 'subsample': 0.7488515680732578}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters found for the pipeline\n",
    "best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d6873",
   "metadata": {},
   "source": [
    "Test the performance of the \"best\" pipeline by training it on the entire training (trn + val) set once and then testing it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "908aef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_pos</td>\n",
       "      <td>15.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>78.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>78.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auroc</td>\n",
       "      <td>88.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ap</td>\n",
       "      <td>87.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>precision</td>\n",
       "      <td>86.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>recall</td>\n",
       "      <td>68.421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>f1_score</td>\n",
       "      <td>76.471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric   Value\n",
       "0              n_pos  15.000\n",
       "1           accuracy  78.947\n",
       "2  balanced_accuracy  78.947\n",
       "3              auroc  88.643\n",
       "4                 ap  87.370\n",
       "5          precision  86.667\n",
       "6             recall  68.421\n",
       "7           f1_score  76.471"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_performance(params, X_trn, y_trn, X_tst, y_tst):\n",
    "    best_model = build_pipeline(params)\n",
    "    best_model.fit(X_trn, y_trn)\n",
    "    y_score = best_model.predict_proba(X_tst)[:,1]\n",
    "    y_pred = best_model.predict(X_tst)\n",
    "    return binary_classification_metrics(y_tst, y_pred, y_score)\n",
    "\n",
    "k_fold_test_metrics = test_performance(best_trial.params, train_X, train_y, X_tst, y_tst)\n",
    "k_fold_test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a14519",
   "metadata": {},
   "source": [
    "Train one final time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_pipeline(best_trial.params)\n",
    "best_model.fit(pd.concat([train_X, X_tst]), pd.concat([train_y, y_tst]))\n",
    "\n",
    "### Save model\n",
    "# with open(f\"{DATA_DIR}/best_pipeline_pk.pkl\", \"wb\") as f:\n",
    "#     pickle.dump({\n",
    "#         \"test_metrics\": k_fold_test_metrics,\n",
    "#         \"trained_pipeline\": best_model\n",
    "#     }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863ae55",
   "metadata": {},
   "source": [
    "You should save the best pipeline and metrics for inference on other datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
